"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[7078],{1184(e,l,n){n.d(l,{R:()=>r,x:()=>a});var s=n(4041);const i={},o=s.createContext(i);function r(e){const l=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(l):{...l,...e}},[l,e])}function a(e){let l;return l=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(o.Provider,{value:l},e.children)}},7333(e,l,n){n.r(l),n.d(l,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"ai/local-llm","title":"Local LLM","description":"Run AI models locally using llama.cpp for complete privacy.","source":"@site/docs/ai/local-llm.md","sourceDirName":"ai","slug":"/ai/local-llm","permalink":"/docs/docs/ai/local-llm","draft":false,"unlisted":false,"editUrl":"https://github.com/protofy-xyz/protofy/tree/main/apps/docs/docs/ai/local-llm.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"AI Integration","permalink":"/docs/docs/category/ai-integration"},"next":{"title":"OpenAI Integration","permalink":"/docs/docs/ai/openai"}}');var i=n(1085),o=n(1184);const r={},a="Local LLM",d={},c=[{value:"Setup",id:"setup",level:2},{value:"Using the AI Wizard (Recommended)",id:"using-the-ai-wizard-recommended",level:3},{value:"Adding Custom Models",id:"adding-custom-models",level:3},{value:"Usage in Cards",id:"usage-in-cards",level:2},{value:"Preloading",id:"preloading",level:2},{value:"Status Check",id:"status-check",level:2},{value:"GPU Acceleration",id:"gpu-acceleration",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Server Crashes",id:"server-crashes",level:3},{value:"Slow Performance",id:"slow-performance",level:3},{value:"GPU Issues on Windows",id:"gpu-issues-on-windows",level:3}];function t(e){const l={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(l.header,{children:(0,i.jsx)(l.h1,{id:"local-llm",children:"Local LLM"})}),"\n",(0,i.jsx)(l.p,{children:"Run AI models locally using llama.cpp for complete privacy."}),"\n",(0,i.jsx)(l.h2,{id:"setup",children:"Setup"}),"\n",(0,i.jsx)(l.h3,{id:"using-the-ai-wizard-recommended",children:"Using the AI Wizard (Recommended)"}),"\n",(0,i.jsxs)(l.ol,{children:["\n",(0,i.jsxs)(l.li,{children:["Go to ",(0,i.jsx)(l.code,{children:"/workspace/settings"})," \u2192 ",(0,i.jsx)(l.strong,{children:"AI"})]}),"\n",(0,i.jsxs)(l.li,{children:["Select ",(0,i.jsx)(l.strong,{children:"Local"})," provider"]}),"\n",(0,i.jsxs)(l.li,{children:["Choose model size (the wizard downloads ",(0,i.jsx)(l.strong,{children:"Gemma 3"})," automatically)"]}),"\n",(0,i.jsx)(l.li,{children:"Wait for download to complete"}),"\n",(0,i.jsx)(l.li,{children:"Start using local AI"}),"\n"]}),"\n",(0,i.jsx)(l.p,{children:"The wizard handles downloading both the llama-server binary and the model file."}),"\n",(0,i.jsx)(l.h3,{id:"adding-custom-models",children:"Adding Custom Models"}),"\n",(0,i.jsx)(l.p,{children:"You can add additional models manually:"}),"\n",(0,i.jsxs)(l.ol,{children:["\n",(0,i.jsxs)(l.li,{children:["Download a ",(0,i.jsx)(l.code,{children:".gguf"})," model file"]}),"\n",(0,i.jsxs)(l.li,{children:["Place it in ",(0,i.jsx)(l.code,{children:"data/models/"})]}),"\n",(0,i.jsx)(l.li,{children:"Select it from the dropdown in AI settings"}),"\n"]}),"\n",(0,i.jsx)(l.p,{children:(0,i.jsx)(l.strong,{children:"Compatible models:"})}),"\n",(0,i.jsxs)(l.ul,{children:["\n",(0,i.jsx)(l.li,{children:"Gemma 3 (various sizes)"}),"\n",(0,i.jsx)(l.li,{children:"LLaMA 3"}),"\n",(0,i.jsx)(l.li,{children:"Mistral"}),"\n",(0,i.jsx)(l.li,{children:"Any GGUF-format model"}),"\n"]}),"\n",(0,i.jsx)(l.h2,{id:"usage-in-cards",children:"Usage in Cards"}),"\n",(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-javascript",children:"// List available models\nconst models = await context.llama.llamaListModels()\n\n// Query local model\nconst response = await context.llama.prompt({\n    message: 'Analyze this data',\n    model: 'gemma-3-12b-it-Q4_1'  // without .gguf\n})\n"})}),"\n",(0,i.jsx)(l.h2,{id:"preloading",children:"Preloading"}),"\n",(0,i.jsx)(l.p,{children:"For faster first response, preload the model:"}),"\n",(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-javascript",children:"await context.llama.llamaPreload('gemma-3-12b-it-Q4_1')\n"})}),"\n",(0,i.jsx)(l.h2,{id:"status-check",children:"Status Check"}),"\n",(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-javascript",children:"const status = await context.llama.llamaStatus()\n// { serverRunning: true, modelLoaded: 'gemma-3-12b-it-Q4_1', ... }\n"})}),"\n",(0,i.jsx)(l.h2,{id:"gpu-acceleration",children:"GPU Acceleration"}),"\n",(0,i.jsx)(l.p,{children:"llama.cpp automatically uses GPU if available:"}),"\n",(0,i.jsxs)(l.ul,{children:["\n",(0,i.jsxs)(l.li,{children:[(0,i.jsx)(l.strong,{children:"NVIDIA"}),": CUDA support"]}),"\n",(0,i.jsxs)(l.li,{children:[(0,i.jsx)(l.strong,{children:"AMD"}),": ROCm support"]}),"\n",(0,i.jsxs)(l.li,{children:[(0,i.jsx)(l.strong,{children:"Apple"}),": Metal support"]}),"\n"]}),"\n",(0,i.jsx)(l.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(l.h3,{id:"server-crashes",children:"Server Crashes"}),"\n",(0,i.jsxs)(l.ol,{children:["\n",(0,i.jsxs)(l.li,{children:["Check ",(0,i.jsx)(l.code,{children:"logs/raw/core*.log"})," for errors"]}),"\n",(0,i.jsx)(l.li,{children:"Try a smaller model"}),"\n",(0,i.jsxs)(l.li,{children:["Ensure model file is valid ",(0,i.jsx)(l.code,{children:".gguf"})]}),"\n"]}),"\n",(0,i.jsx)(l.h3,{id:"slow-performance",children:"Slow Performance"}),"\n",(0,i.jsxs)(l.ol,{children:["\n",(0,i.jsx)(l.li,{children:"Use smaller quantized model (Q4 vs Q8)"}),"\n",(0,i.jsx)(l.li,{children:"Close other GPU-intensive apps"}),"\n"]}),"\n",(0,i.jsx)(l.h3,{id:"gpu-issues-on-windows",children:"GPU Issues on Windows"}),"\n",(0,i.jsxs)(l.p,{children:["The process manager includes cleanup hooks for GPU resources. Always use ",(0,i.jsx)(l.code,{children:"yarn stop"})," to properly release GPU memory."]})]})}function h(e={}){const{wrapper:l}={...(0,o.R)(),...e.components};return l?(0,i.jsx)(l,{...e,children:(0,i.jsx)(t,{...e})}):t(e)}}}]);